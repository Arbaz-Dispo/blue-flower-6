name: Nevada Business Scraper

on:
  workflow_dispatch:  # Allows manual triggering
    inputs:
      file_number:
        description: 'Nevada business file number to search for (e.g., "E10281132020-8")'
        required: true
        default: 'E10281132020-8'
      request_id:
        description: 'Unique request ID (for tracking concurrent requests)'
        required: false
        default: ''
      test_run:
        description: 'Test run (optional)'
        required: false
        default: 'false'
      force_rebuild:
        description: 'Force rebuild Docker image'
        required: false
        default: 'false'
        type: boolean
  push:
    paths:
      - 'Dockerfile'  # Rebuild image when Dockerfile changes
      - 'requirements.txt'  # Rebuild when dependencies change
  schedule:
    # Run daily at 10 AM UTC (optional - remove if you don't want scheduled runs)
    - cron: '0 10 * * *'

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/nevada-scraper

jobs:
  # Build and push Docker image (only when needed)
  build-image:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event.inputs.force_rebuild == 'true'
    permissions:
      contents: read
      packages: write
    outputs:
      image-tag: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx with aggressive caching
      uses: docker/setup-buildx-action@v3
      with:
        driver-opts: |
          image=moby/buildkit:buildx-stable-1
          network=host
        
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Build and push with maximum caching
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
        cache-from: |
          type=gha
          type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:cache
        cache-to: |
          type=gha,mode=max
          type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:cache,mode=max
        platforms: linux/amd64
        build-args: |
          BUILDKIT_INLINE_CACHE=1

  # Run the Nevada scraping process
  scrape-nevada-business:
    runs-on: ubuntu-latest
    needs: [build-image]
    if: always() && (needs.build-image.result == 'success' || needs.build-image.result == 'skipped')
    permissions:
      contents: read
      packages: read
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      

    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Determine Docker image to use
      id: image
      run: |
        # Always use the latest tag for simplicity
        IMAGE_TAG="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest"
        echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
        echo "Using Docker image: $IMAGE_TAG"
        
    - name: Optimized Docker setup (targeting 10-15 seconds)
      run: |
        IMAGE_TAG="${{ steps.image.outputs.image_tag }}"
        echo "🚀 Optimized Docker setup: $IMAGE_TAG"
        
        # Enable Docker BuildKit for faster operations
        export DOCKER_BUILDKIT=1
        
        # Try multiple strategies for fastest pull
        echo "📦 Attempting pull strategies..."
        
        # Strategy 1: Direct pull (usually fastest if image exists)
        if timeout 45s docker pull "$IMAGE_TAG" 2>/dev/null; then
          echo "✅ Pre-built image pulled successfully"
          echo "IMAGE_TAG=$IMAGE_TAG" >> $GITHUB_ENV
          
        # Strategy 2: Check if we can build with cache faster than pull
        elif [ -f "Dockerfile" ]; then
          echo "🔨 Building with cache (may be faster than remote pull)..."
          docker build \
            --cache-from "$IMAGE_TAG" \
            --build-arg BUILDKIT_INLINE_CACHE=1 \
            -t nevada-scraper:local \
            . || docker build -t nevada-scraper:local .
          echo "IMAGE_TAG=nevada-scraper:local" >> $GITHUB_ENV
          
        else
          echo "❌ No Dockerfile found and pull failed"
          exit 1
        fi
        
        echo "✅ Docker setup complete!"
        
    - name: Parse file number
      id: parse-file
      run: |
        FILE_NUMBER="${{ github.event.inputs.file_number || 'E10281132020-8' }}"
        REQUEST_ID="${{ github.event.inputs.request_id || '' }}"
        
        # Generate a fallback request ID if none provided
        if [ -z "$REQUEST_ID" ]; then
          REQUEST_ID="nevada-${{ github.run_number }}-$(date +%s)"
        fi
        
        echo "file_number=$FILE_NUMBER" >> $GITHUB_OUTPUT
        echo "request_id=$REQUEST_ID" >> $GITHUB_OUTPUT
        echo "Processing file number: $FILE_NUMBER with request ID: $REQUEST_ID"
        
    - name: Run Nevada scraper
      id: scraper
      env:
        SOLVECAPTCHA_API_KEY: ${{ secrets.SOLVECAPTCHA_API_KEY }}
        FILE_NUMBER: ${{ steps.parse-file.outputs.file_number }}
        REQUEST_ID: ${{ steps.parse-file.outputs.request_id }}
      run: |
        echo "🚀 Starting Nevada scraper..."
        
        # Run with minimal overhead
        docker run --rm \
          --name nevada-scraper-${{ github.run_number }} \
          -e SOLVECAPTCHA_API_KEY="${SOLVECAPTCHA_API_KEY}" \
          -e FILE_NUMBER="${FILE_NUMBER}" \
          -e REQUEST_ID="${REQUEST_ID}" \
          -e PYTHONUNBUFFERED=1 \
          -v "$(pwd):/workspace" \
          -w /workspace \
          --memory="2g" \
          --cpus="1.5" \
          "$IMAGE_TAG" \
          timeout 300s python nevada_scraper.py
        
    - name: List generated files
      run: |
        echo "Files in workspace:"
        ls -la
        echo "Looking for JSON files:"
        find . -name "*.json" -type f
        
    - name: Upload scraped data as artifact
      uses: actions/upload-artifact@v4
      with:
        name: nevada-scraped-data-${{ steps.parse-file.outputs.request_id }}-${{ steps.parse-file.outputs.file_number }}
        path: |
          scraped_data_*.json
          nevada_entity_search.html
        retention-days: 30
        if-no-files-found: warn
        
    - name: Show scraping summary
      run: |
        echo "=== NEVADA REQUEST ${{ steps.parse-file.outputs.request_id }} SCRAPING SUMMARY ==="
        echo "File number processed: ${{ steps.parse-file.outputs.file_number }}"
        
        if [ -f scraped_data_*.json ]; then
          for file in scraped_data_*.json; do
            if [ -f "$file" ]; then
              echo "Scraped data file: $file"
              echo "File size: $(wc -c < "$file") bytes"
              
              # Try to extract summary from the JSON structure
              if command -v jq >/dev/null 2>&1; then
                echo "Metadata:"
                jq -r '.metadata // empty' "$file" 2>/dev/null || echo "  No metadata found"
                
                echo "Entity Information:"
                jq -r '.entity_information.entity_name // "N/A"' "$file" 2>/dev/null || echo "  Entity name: N/A"
                jq -r '.entity_information.entity_status // "N/A"' "$file" 2>/dev/null || echo "  Entity status: N/A"
                
                OFFICER_COUNT=$(jq '.officers | length // 0' "$file" 2>/dev/null || echo "0")
                echo "📊 Number of officers: $OFFICER_COUNT"
                
                SUCCESS_STATUS=$(jq -r '.metadata.success // false' "$file" 2>/dev/null || echo "false")
                echo "✅ Scraping successful: $SUCCESS_STATUS"
              else
                echo "jq not available, showing file size only"
              fi
              echo "📄 Output format: JSON"
              echo "---"
            fi
          done
        else
          echo "No scraped data files found"
        fi
        echo "==========================" 
